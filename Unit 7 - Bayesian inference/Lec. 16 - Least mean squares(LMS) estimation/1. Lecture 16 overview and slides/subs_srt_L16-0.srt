1
00:00:00,000 --> 00:00:00,610

2
00:00:00,610 --> 00:00:03,100
We have seen so far two ways of

3
00:00:03,100 --> 00:00:05,810
estimating an unknown parameter.

4
00:00:05,810 --> 00:00:08,470
We can use the maximum a
posteriori probability

5
00:00:08,470 --> 00:00:12,110
estimate, or we can use the
conditional expectation.

6
00:00:12,110 --> 00:00:16,350
That is, the mean of the
posterior distribution.

7
00:00:16,350 --> 00:00:19,900
These were, in some sense,
arbitrary choices.

8
00:00:19,900 --> 00:00:23,350
How about imposing a performance
criterion, and

9
00:00:23,350 --> 00:00:26,890
then finding an estimate which
is optimal with respect to

10
00:00:26,890 --> 00:00:28,720
that criterion?

11
00:00:28,720 --> 00:00:33,000
This is what we will be
doing in this lecture.

12
00:00:33,000 --> 00:00:36,310
We introduce a specific
performance criterion, the

13
00:00:36,310 --> 00:00:40,330
expected value of the squared
estimation error, and we look

14
00:00:40,330 --> 00:00:44,910
for an estimator that is optimal
under this criterion.

15
00:00:44,910 --> 00:00:48,510
It turns out that the optimal
estimator is the conditional

16
00:00:48,510 --> 00:00:49,850
expectation.

17
00:00:49,850 --> 00:00:53,370
And this is why we have been
calling it the least mean

18
00:00:53,370 --> 00:00:55,770
squares estimator.

19
00:00:55,770 --> 00:00:59,230
It plays a central role because
it is a canonical way

20
00:00:59,230 --> 00:01:02,230
of estimating unknown
random variables.

21
00:01:02,230 --> 00:01:06,010
We will study some of its
theoretical properties, and we

22
00:01:06,010 --> 00:01:09,490
will also illustrate its use and
the associated performance

23
00:01:09,490 --> 00:01:12,020
analysis in the context
of an example.

24
00:01:12,020 --> 00:01:13,270